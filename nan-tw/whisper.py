import os
import numpy as np
import pandas as pd
import librosa
import torch
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
from datasets import Dataset
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import gradio as gr
import evaluate

# 1. è¨­å®šè³‡æ–™å¤¾è·¯å¾‘ï¼ˆè«‹æ”¹æˆä½ æœ¬æ©Ÿçš„è·¯å¾‘ï¼‰
data_dir = "C:/Users/USER/Desktop/nan-tw"   # nan-tw è³‡æ–™å¤¾ï¼Œå…§å« clips å’Œ train.tsv

# 2. è®€å– train/test.tsv
df = pd.read_csv(os.path.join(data_dir, "test.tsv"), sep="\t")

# 3. ä¿®æ­£ Windows è·¯å¾‘ï¼ˆç¢ºä¿ç”¨æ­£æ–œç·šï¼‰
df["path"] = df["path"].str.replace("\\", "/", regex=False)

# 4. ç”¢ç”Ÿå®Œæ•´éŸ³è¨Šè·¯å¾‘
df["audio"] = df["path"].apply(lambda x: os.path.join(data_dir, "clips", x.split('/')[-1]))
df["text"] = df["sentence"]
df = df[["audio", "text"]]

# 5. åˆ†å‰²è¨“ç·´èˆ‡é©—è­‰é›†
split = int(len(df) * 0.9)
df_train = df[:split].reset_index(drop=True)
df_val = df[split:].reset_index(drop=True)

# 6. å»ºç«‹ Dataset ç‰©ä»¶
train_ds = Dataset.from_pandas(df_train)
val_ds = Dataset.from_pandas(df_val)


import torch

def data_collator(features):
    # å±•é–‹ input_features (Whisper æ˜¯ batch["input_features"][0] è£¡é¢æ˜¯ tensor)
    input_features = [torch.tensor(f["input_features"]) if not isinstance(f["input_features"], torch.Tensor) else f["input_features"] for f in features]
    
    # labels æ˜¯ list[int]ï¼Œéœ€è£œé½Šé•·åº¦
    label_features = [torch.tensor(f["labels"], dtype=torch.long) for f in features]
    
    # padding labelsï¼ˆè£œ -100ï¼Œä»£è¡¨ä¸è¨ˆç®— lossï¼‰
    labels_padded = torch.nn.utils.rnn.pad_sequence(label_features, batch_first=True, padding_value=-100)
    
    batch = {
        "input_features": torch.stack(input_features),
        "labels": labels_padded
    }
    return batch

processor = AutoProcessor.from_pretrained("Jobaula/whisper-medium-nan-tw-common-voice")
model = AutoModelForSpeechSeq2Seq.from_pretrained("Jobaula/whisper-medium-nan-tw-common-voice")

#Jobaula/whisper-medium-nan-tw-common-voice
# åŠ å…¥ forced_decoder_idsï¼Œé–å®šèªè¨€èˆ‡ä»»å‹™
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
    task="transcribe"
)


# 8. æª¢æŸ¥ GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)
model.to(device)

# 9. å®šç¾© preprocess å‡½æ•¸ï¼ˆåŠ å…¥æª”æ¡ˆå­˜åœ¨æª¢æŸ¥ï¼‰
def preprocess(batch):
    audio_path = batch["audio"]
    audio, _ = librosa.load(audio_path, sr=16000)

    # ç‰¹å¾µæå–ï¼ˆWhisper expects log-mel spectrogramsï¼‰
    features = processor.feature_extractor(audio, sampling_rate=16000).input_features[0]

    # åŠ ä¸Šæœ€å¤§é•·åº¦é™åˆ¶ï¼ˆWhisper æœ€å¤§è¼¸å‡ºé•·åº¦ç‚º 448ï¼‰
    labels = processor.tokenizer(
        batch["text"],
        max_length=448,
        padding="max_length",   # å¦‚æœä½ è¦ç”¨ Trainerï¼Œé€™æ¨£å°é½Š padding
        truncation=True         # é‡é»ï¼šæˆªæ–·éé•·çš„æ–‡å­—
    ).input_ids

    return {
        "input_features": features,
        "labels": labels
    }




# 10. ä½¿ç”¨ map é è™•ç† Dataset
train_ds = train_ds.map(preprocess, remove_columns=train_ds.column_names)
val_ds = val_ds.map(preprocess, remove_columns=val_ds.column_names)

# 11. è¨­å®šè¨“ç·´åƒæ•¸
training_args = Seq2SeqTrainingArguments(
  output_dir="./whisper-nan-tw",
    per_device_train_batch_size=1,            # å°ä¸€é»ï¼ŒåŠ å¿«æ¯æ­¥é€Ÿåº¦
    gradient_accumulation_steps=4,            # é€™æ¨£ç­‰æ–¼æœ‰æ•ˆ batch size = 8
    learning_rate=5e-2,                       # æ¯” 1e-2 å°å¾ˆå¤šï¼Œå° Whisper æ›´ç©©
    warmup_steps=30,                          # åªé ç†±ä¸€å°æ®µå³å¯
    max_steps=5,                            # æ¸¬è©¦æ•ˆæœå°±å¥½ï¼Œå…ˆè¨“ç·´ 100 steps
    logging_steps=5,                          # æ›´å¯†é›†è§€å¯Ÿ loss
    save_steps=5,                           # åªå­˜ä¸€æ¬¡ checkpoint
    save_total_limit=1,
    fp16=torch.cuda.is_available(),           # æœ‰ GPU æ‰é–‹ fp16
    report_to="none"

)

# 12. è¼‰å…¥è©•ä¼°æŒ‡æ¨™
wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")
import logging
logging.basicConfig(filename='metrics.log', level=logging.INFO)   

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

   
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    logging.info(f"WER: {wer}, CER: {cer}")
    return {
        "wer": wer_metric.compute(predictions=pred_str, references=label_str),
        "cer": cer_metric.compute(predictions=pred_str, references=label_str),
    }

# 13. å»ºç«‹ Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    data_collator=data_collator,  
    compute_metrics=compute_metrics,
    tokenizer=processor.tokenizer
)

# 14. é–‹å§‹è¨“ç·´
# é–‹å§‹è¨“ç·´
trainer.train()

# âœ… å„²å­˜è¨“ç·´å¾Œçš„æ¨¡å‹åˆ°æŒ‡å®šè³‡æ–™å¤¾
trainer.save_model("./whisper-nan-tw/final-checkpoint")

# âœ… å„²å­˜ processor å’Œ tokenizerï¼ˆé€™æ¨£æ¨è«–æ‰ä¸æœƒå‡ºéŒ¯ï¼‰
processor.save_pretrained("./whisper-nan-tw/final-checkpoint")
if hasattr(processor, "tokenizer"):
    processor.tokenizer.save_pretrained("./whisper-nan-tw/final-checkpoint")

    # ğŸ”š è¨“ç·´çµæŸå¾Œæ‰‹å‹•è·‘ä¸€æ¬¡è©•ä¼°ï¼ˆä¸¦è¼¸å‡ºçµæœï¼‰
metrics = trainer.evaluate()
print("Final Eval Metrics:", metrics)

